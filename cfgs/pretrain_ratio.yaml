optimizer : {
  type: AdamW,
  kwargs: {
  lr : 0.0001,
  weight_decay : 0.05
}}

scheduler: {
  type: CosLR,
  kwargs: {
    epochs: 300,
    initial_epochs : 10
}}

dataset : {
  train : { _base_: cfgs/dataset_configs/AsEP.yaml,
            others: {subset: 'train'}},
  val : { _base_: cfgs/dataset_configs/AsEP.yaml,
            others: {subset: 'val'}},
  test : { _base_: cfgs/dataset_configs/AsEP.yaml,
            others: {subset: 'test'}}}

model : {
  NAME: Point_MAE_Epitope_Pretrain,
  transformer_config: {
    trans_dim: 256,
    encoder_dims: 256,
    depth: 4,
    drop_path_rate: 0.3,
    num_heads: 4
  },
  mask_ratio: 0.50,
  aa_weight: 1,
  solvent_weight: 20,
  pssm_weight: 5,
  lora_rank: 0,
  mask_ratio_for_zero: 0.2,
  }

total_bs : 96 
step_per_update : 1
max_epoch : 300
